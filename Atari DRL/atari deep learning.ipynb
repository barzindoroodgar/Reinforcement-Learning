{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()\n",
    "\n",
    "ATARI_SHAPE = (84, 84, 4)\n",
    "ACTION_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# deep mind model \n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "def atari_model():\n",
    "    frames_input = layers.Input(ATARI_SHAPE, name='inputs')\n",
    "    actions_input = layers.Input((ACTION_SIZE,), name='action_mask')\n",
    "\n",
    "    normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "\n",
    "    conv_1 = layers.convolutional.Conv2D(\n",
    "        16, (8, 8), strides=(4, 4), activation='relu')(normalized)\n",
    "    conv_2 = layers.convolutional.Conv2D(\n",
    "        32, (4, 4), strides=(2, 2), activation='relu')(conv_1)\n",
    "    conv_flattened = layers.core.Flatten()(conv_2)\n",
    "    hidden = layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    output = layers.Dense(ACTION_SIZE)(hidden)\n",
    "    filtered_output = layers.Multiply(name='QValue')([output, actions_input])\n",
    "\n",
    "    model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "    model.summary()\n",
    "    optimizer = RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "    model.compile(optimizer, loss=huber_loss)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epslion_greedy_policy_action(current_state,episode):\n",
    "    if np.random.rand()<=epslion or episode<total_observe_count:\n",
    "        #take random action\n",
    "        return random.randrange(ACTION_SIZE)\n",
    "    else:\n",
    "        #take the best action\n",
    "        Q_value = model.predict([current_state,np.ones(ACTION_SIZE).reshape(1, ACTION_SIZE)])\n",
    "        return np.argmax(Q_value[0])\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def huber_loss(y, q_value):\n",
    "    error = K.abs(y - q_value)\n",
    "    quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "    linear_part = error - quadratic_part\n",
    "    loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "    return loss\n",
    "\n",
    "def get_sample_random_batch_from_replay_memory():\n",
    "    \n",
    "    mini_batch = random.sample(replay_memory,batch_size)\n",
    "    \n",
    "    current_state_batch = np.zeros((batch_size, 84,84, 4))\n",
    "    next_state_batch = np.zeros((batch_size, 84,84, 4))\n",
    "   \n",
    "    \n",
    "    actions, rewards, dead = [], [], []\n",
    "    \n",
    "    for idx, val in enumerate(mini_batch):\n",
    "        \n",
    "        current_state_batch[idx] = val[0]\n",
    "        actions.append(val[1])\n",
    "        rewards.append(val[2])\n",
    "        next_state_batch[idx] = val[3]\n",
    "        dead.append(val[4])\n",
    "    \n",
    "    return current_state_batch , actions, rewards, next_state_batch, dead\n",
    "\n",
    "\n",
    "def deepQlearn():\n",
    "    \n",
    "    current_state_batch , actions, rewards, next_state_batch, dead = get_sample_random_batch_from_replay_memory()\n",
    "    \n",
    "    actions_mask = np.ones((batch_size,ACTION_SIZE))\n",
    "    next_Q_values = target_model.predict([next_state_batch,actions_mask])  # separate old model to predict\n",
    "    \n",
    "    targets = np.zeros((batch_size,))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        if dead[i]:\n",
    "            targets[i] = -1\n",
    "        else:\n",
    "            targets[i] = rewards[i] + gamma * np.amax(next_Q_values[i])\n",
    "            \n",
    "    one_hot_actions=np.eye(ACTION_SIZE)[np.array(actions).reshape(-1)]\n",
    "    one_hot_targets = one_hot_actions * targets[:, None]\n",
    "    \n",
    "    model.fit([current_state_batch,one_hot_actions], one_hot_targets, epochs=1,batch_size=batch_size, verbose=0)\n",
    "    \n",
    "def save_model(episode):\n",
    "    model_name = \"atari_model{}.h5\".format(episode)\n",
    "    model.save(model_name)\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "\n",
    "#preprocessing the image\n",
    "def pre_process(frame_array):\n",
    "    \n",
    "    #converting into graysclae since colors don't matter\n",
    "    \n",
    "    from skimage.color import rgb2gray\n",
    "    grayscale_frame = rgb2gray(frame_array)\n",
    "    \n",
    "    # resizing the image \n",
    "    from skimage.transform import resize\n",
    "    resized_frame = np.uint8(resize(grayscale_frame, (84, 84), mode='constant') * 255)\n",
    "    \n",
    "#     return np.reshape([resized_frame], (1, 84, 84, 1))\n",
    "    return resized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 20, 20, 16)   4112        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 9, 9, 32)     8224        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2592)         0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          663808      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 3)            771         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 3)            0           dense_8[0][0]                    \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 676,915\n",
      "Trainable params: 676,915\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 20, 20, 16)   4112        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 9, 9, 32)     8224        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2592)         0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          663808      flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 3)            771         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 3)            0           dense_10[0][0]                   \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 676,915\n",
      "Trainable params: 676,915\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "replay_memory = deque(maxlen=400000)\n",
    "model = atari_model()\n",
    "target_model = atari_model()\n",
    "\n",
    "nEpisodes = 1000\n",
    "total_observe_count = 750\n",
    "epslion = 1.0\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "final_epsilon = 0.1\n",
    "epsilon_step_num = 1000\n",
    "epsilon_decay = (1.0 - final_epsilon) / epsilon_step_num\n",
    "max_score = 0\n",
    "target_model_change = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score for the episode 0 is : 0.0 \n",
      "max score for the episode 1 is : 1.0 \n",
      "max score for the episode 1 is : 2.0 \n",
      "max score for the episode 16 is : 3.0 \n",
      "max score for the episode 65 is : 4.0 \n",
      "max score for the episode 65 is : 5.0 \n",
      "max score for the episode 65 is : 6.0 \n",
      "final score for the episode 100 is : 0.0 \n",
      "final score for the episode 200 is : 1.0 \n",
      "final score for the episode 300 is : 2.0 \n",
      "max score for the episode 350 is : 7.0 \n",
      "final score for the episode 400 is : 0.0 \n",
      "final score for the episode 500 is : 3.0 \n",
      "final score for the episode 600 is : 2.0 \n",
      "final score for the episode 700 is : 2.0 \n",
      "max score for the episode 789 is : 8.0 \n",
      "max score for the episode 789 is : 9.0 \n",
      "final score for the episode 800 is : 2.0 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(nEpisodes):\n",
    "    \n",
    "    dead, done, lives_remaining,score = False, False, 5, 0\n",
    "    \n",
    "    current_state = env.reset()\n",
    "    for _ in range(random.randint(1, 30)):\n",
    "        current_state, _, _, _ = env.step(1)\n",
    "        \n",
    "    current_state = pre_process(current_state)\n",
    "    current_state = np.stack((current_state, current_state, current_state, current_state), axis=2)\n",
    "    current_state = np.reshape([current_state], (1, 84, 84, 4))\n",
    "    \n",
    "    while not done:\n",
    "    \n",
    "        action = epslion_greedy_policy_action(current_state,episode)\n",
    "        real_action = action + 1\n",
    "        \n",
    "        if epslion > final_epsilon and episode > total_observe_count:\n",
    "                epslion -= epsilon_decay\n",
    "        \n",
    "        next_state, reward, done, lives_left = env.step(real_action)\n",
    "        \n",
    "        next_state = pre_process(next_state) # 84,84 grascale frame \n",
    "        next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "        next_state = np.append(next_state, current_state[:, :, :, :3], axis=3)\n",
    "        \n",
    "        if lives_remaining > lives_left['ale.lives']:\n",
    "            dead = True\n",
    "            lives_remaining = lives_left['ale.lives']\n",
    "\n",
    "        \n",
    "        replay_memory.append((current_state, action, reward, next_state, dead))\n",
    "        \n",
    "        if episode>total_observe_count:\n",
    "            deepQlearn()\n",
    "            \n",
    "            if episode % target_model_change == 0:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "      \n",
    "            \n",
    "        score += reward\n",
    "        \n",
    "        if dead:\n",
    "            dead = False\n",
    "        else:\n",
    "            current_state = next_state\n",
    "            \n",
    "        if max_score<score:\n",
    "            print(\"max score for the episode {} is : {} \".format(episode,score))\n",
    "            max_score = score\n",
    "            \n",
    "    if episode%100 == 0:\n",
    "        print(\"final score for the episode {} is : {} \".format(episode,score))\n",
    "        save_model(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
